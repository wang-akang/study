---
title: "对比学习论文综述【论文精读】"
date: 2025-6-17
---





----------




对比学习在计算机视觉领域的发展历程，4个阶段：

1.  百花齐放：方法、模型、目标函数、代理任务都还没有统一。
2.  CV双雄：MOCOv1、SimCLRv1、MOCOv2、SimCLRv2、CPC和CMC的延伸工作、SwaV，这个阶段发展非常迅速，以上这些工作间隔时间都很短，ImageNet上的最好成绩，基本上每个月都在被刷新。
3.  不用负样本：BYOL及其后续改进，SimSima把所有方法都归纳总结，融入到SimSima框架之中，算是卷积神经网络做对比学习的总结性工作。
4.  transformer：MOCOv3、DINO，用vision transformer开展工作。

阶段一：百花齐放 18年-19年中

01:51instdisc：提出了代理任务：个体判别任务。

就是MOCO中反复提到的文献61，如果MOCO是一篇里程碑式的工作，那么InstDisc就是巨人的肩膀，就是MOCO提到的memory bank方法的论文。

创新点：用个体判别+NCEloss，做对比学习，取得了不错的无监督表征学习的结果。同时它还提出了用别的数据结构，去存大量的负样本。以及如何对特征进行动量的更新。

对后来的对比学习的工作气到了至关重要的推进作用。

**图1，动机** 受到有监督学习结果的启发：让相似图片聚集在一起的原因并不是他们有相似的语义标签，而是图片确实长得太像了。基于此提出了个体判别任务，这种无监督学习方式，就是把按类别走的有监督信号推到了极致，就是把每个instance（图片）都看成一个类别，目标是能学一种特征，让我们能把每一张图片都区分开。

**图2，方法** 简单来说就是想通过一个CNN把图片都编码成一个特征，希望这个特征能在最后的特征空间中尽可能的分开（每个图片都是自己的类）；训练CNN：对比学习，正样本是图片本身，负样本是数据集中所有其他的图片（用memory bank存放大量的负样本）

**前向过程：**

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077669.jpg)

bs：256（256个图片输入，也是正样本）

编码器：res50（生成2048维度的特征）

降维：128维（每个图片的特征大小）

负样本：从memory bank中随机抽（文中抽了4096个负样本）

用NCEloss去算对比学习的目标函数，一旦更新完网络，就用这个minibatch中的数据样本所对应的特征，去更换原来memory bank中的特征，这样memory bank就得到了更新。反复这个过程，不停的更新memory bank，让最后学到的特征尽可能的有区分性。

3.3\. Proximal Regularization

给模型训练加了一个约束，让memory bank中的特征进行动量式的更新

4.2 实验超参数的设定

算loss的时候，温度设置是0.07，选了4000个负样本，训练200个epoch，bs是256，起始的lr是0.03（MOCO延续了这些超参数的设定）

07:05InvaSpread：可以看做SimCLR的前身，没有使用额外的数据结构去存储大量的负样本，正负样本来自同一个minibatch，而且只用一个编码器进行端到端的学习。

端到端学习、一个编码器、不需要外部数据结构去存储大量的负样本、正负样本来自同一个minibatch。

没有TPU，所以bs设置为256，负样本数量只有（256-1）×2个；缺少SimCLR强大的数据增广；SimCLR提出的mlp projector；所以效果不如SimCLR。

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077673.jpg)

对比学习的思想：同样的图片通过编码器，特征应该很类似，不同的图片特征应该不类似。

Invariant：相似的图片，特征应该保持不变性

Spreading：不相似的图片和物体，特征应该尽可能的分散开

具体做法：

*   代理任务：个体判别
*   正负样本的选取：

根据前向过程：

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077674.jpg)

1.  256张图片经过数据增强，又得到了256张图片。正样本是当前图片数据增强后对应的图片，其余所有图片都是负样本（包括原始图片和经过数据增强后的图片）对于一个batch来说，正样本：256，负样本：（256-1）×2。【和InstDisc不同，InstDisc正样本256，但负样本是从一个memory bank中抽出来的，负样本是4096，甚至可以更大】为了能用一个编码器做端到端的训练，本文从同一个minibatch中选正负样本。【这就是MOCO中讲到的端到端的学习方式】
2.  编码器+FC（降维），相似的特征在特征空间中尽可能的接近，与其他的特征应该尽可能的拉远。目标函数式NCEloss的变体。

10:33CPC：contrastive predivtive coding，一个可以处理音频、图片、文字还可以使用在强化学习中的通用模型。用预测的代理任务做对比学习。

【机器学习：判别式模型和生成式模型】

个体判别任务：前两个工作都是判别式的代理任务

预测型的任务：最常见的生成式的代理任务

输入：音频信号

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077751.jpg)

大概想法：有一个持续的序列，把之前时刻的输入给编码器，得到一些特征，把这些特征喂给一个自回归的模型（auto regression，常见的自回归模型就是RNN、LSTM），得到红色的方块（context representation一个代表上下文的特征表示），如果这个上下文的特征表示足够好，就是它真的包含了当前和之前所有的信息的话，那它应该可以做出一些合理的预测，用它来预测未来时刻的特征输出。对比学习体现在：正样本是未来时刻的特征输出，预测的特征是query，真正未来时刻的输出是由输入决定的。负样本的定义很广泛，比如，可以任选输入，通过编码器得到输出，那预测应该是不相似的。这套思想是很普适的，输入可以换成句子，用前面的单词去预测后面的单词的特征的输出；可以换成一系列的图片patch，左上到右下，可以用上半部分的图片特征预测下半部分的图片特征。

13:05 CMCccontrastive multiview coding

本文定义正样本的方式很广泛，一个物体的多个视角都可以被当做正样本。

第一个做多视角的对比学习，不仅证明了对比学习的灵活性，也证明了多视角、多模态的可行性。【接下来OpenAI就出了clip模型：如果有一个图片以及描述它的文本，这就是一个正样本对，用来做多模态的对比学习】

摘要精彩：人类观察世界是通过多个传感器的，眼睛、耳朵都充当不同的传感器，给我们的大脑提供不同的信号，每个视角都是带有噪声而且带有噪声的，但是最重要的那些信息是在所有视角中共享的，比如基础的物理定律、几何形状、语义信息都是共享的。例：狗既可以被看见，也可以被听见和感受到。基于这个现象，作者提出，我们想要学习一个非常强大的特征，它具有视角不变性。CMC的目的就是增大所有视角的互信息。如果能学习到一个特征，能抓住所有视角下的关键因素，这个特征就很好了。

正负样本的定义：

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077705.jpg)

数据集：NYU RGBD，有四个视角，原始图像、深度信息（距离观察者的远近）、surface normal（表面法线）、物体的分割图像

正样本：虽然不同的输入来自不同的传感器，或者来自不同的模态，但是所有的输入都对应的一个东西，他们就应该互为正样本，也就是，绿色的点在特征空间中应该非常接近。

负样本：如果随机选择一张图片，不配对的，得到的特征就应该远离。

局限性：在处理不同视角/模态的时候，可能需要不同的编码器，因为不同的输入可能长得很不一样，这样计算代价就会高。比如在CLIP中，文本端用的是bert，图像用的是vit。transformer有可能同时处理不同模态的数据【MA-CLIP: Towards Modality-Agnostic Contrastive Language-Image Pre-training 用一个transformer同时处理两个模态，效果反而更好】不用针对每个数据去做特有的改进。

第一阶段的四篇论文，使用的代理任务不同（个体判别、预测未来、多视角多模态）、目标函数不同（NCE、InfoNCE、NCE的其他变体）、模型不同（InvaSpread仅用一个编码器、InstDisc用一个编码器和memory bank、CPC是一个编码器和一个自回归模型、CMC有两个甚至多个编码器）任务不同（图像、视频、音频、文字、强化学习）

18:12阶段二：CV双雄19年中-20年中

CV双雄指的就是MOCO和SimCLR

18:36 MoCo

主要贡献：把之前的对比学习方法归纳总结成一个字典查询的问题。提出队列和动量更新的编码器构造一个又大又一致的字典，能帮助更好的对比学习。

实现上和InstDisc非常相似，可以说是它的一个改进工作。但是改进简单有效，而且有很大的影响力，这个动量编码器的改进一直沿用到了最新的工作，带来好的效果。

另外moco的写作也很精彩，自顶向下：并没有按照传统简单直白的写作方式，先对比过去的工作，谈局限性，然后提出自己的方法。

而是：引言中，第一段写CV和NLP的区别，以及到底为什么无监督学习在CV这边做的不好；第二段开始讲对比学习，直接把对比学习的方法总结成一个字典查找的问题；然后在CV和NLP大一统、对比学习被看做字典查找也大一统的大框架下，提出了MOCO这个框架，希望能用一个又大又一致的字典，去整体的提高对比学习的性能。

方法部分，没有模型总览图、没有说模型、任务。而是从目标函数入手，说我们用的是InfoNCE，先定义正负样本，然后网络结构，然后实现细节和伪代码。3.1中，为了让MOCO看起来更普适，没有直接定义输入是什么，也没有定义网络结构是什么样

什么样的输入都可以（图片、图片块CPC、上下文的图片块）

网络：query和key的编码器既可以相同（InvaSpread）、部分共享和完全不同的（CMC多个视角所以多个编码器）

23:07 SimCLRsimple contrastive learning ICML

介绍对比学习常用SimCLR当例子，因为它概念上更容易理解，方法也很容易解释，只不过就是bs太大，一般人不好上手。

方法：

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077749.jpg)

*   x：一个minibatch的图片。
*   xi和xj：x经过不同的数据增强，他俩就是正样本。正样本个数就是bs，负样本就是剩下的样本以及他们数据增强后的样本2（bs-1）。
*   f函数：是编码器，两个编码器共享权重。
*   h：编码器得到的特征。
*   g函数：projector，就是一个全连接层跟着一个relu的激活函数。（就是这么的一个简单的错做，能让最后学到的特征在imagenet这个分类任务上提10个点）。只有在训练的时候用，做下游任务的时候，只用特征。这个g函数只是为了让模型训练的更好，为了公平对比在下游任务上不使用。
*   最后衡量（名为normalized temperature-scaled的交叉熵函数：normalized是指在特征后面做了L2归一化，temperature-scaled：在loss上乘一个τ）正样本之间是否能达到最大一致性。

前向过程：图片进入编码器编码，然后projector降维，最后算一个对比学习的loss。

SimCLR和InvaSpread的区别：

1.  SimCLR用了更多的数据增强（**裁剪、改变色彩**、旋转、cutout、高斯噪声、高斯模糊、sobel滤波器）
2.  加了一个g函数（可学习的非线性变换），就是一个MLP层。
3.  用了更大的bs，而且训练的时间更久

SimCLR框架中几乎所有单独的组件都出现在以前的工作中，尽管具体的实现可能有所不同。但SimCLR的优势不是任何单一设计的选择，而是把所有的技术结合起来得到的结果。我们提供了一个全面的比较，非常详细的消融实验，在附录C。

SimCLR中提出的很多技术，都对后续的工作产生了长远的影响：

1.  在编码器之后加一个MLP层（MOCOv2，BYOL）
2.  数据增强技术
3.  用LARS优化器去做大bs的模型训练（BYOL）

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077917.jpg)

*   Linear：不要relu，只要一个全连接层
*   Non-Linear（projection head）：加了relu激活层，把线性函数变成非线性了
*   None：就像MOCO和InvaSpread，直接拿编码器出来的特征去做对比学习

可以发现：Non-Linear提点效果非常显著；特征维度的大小效果没什么区别，选128就够了。

31:03MOCOv2 CV的会议

因为moco和SimCLR的效果实在太好，2020年就掀起了对比学习的狂潮。直到2020年底，vision transformer出来以后才逐渐消退。

发现SimCLR的效果很好，技术都是即插即用型。就在MOCO上做很简单的改动：把MLP projection head和更多的数据增强用起来，就刷新了imagenet上的最好成绩，比过了SimCLR。

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077747.jpg)

具体改进如表1：加了MLP层、更多数据增强、训练的时候用了cosine learning rate schedule、训练更长的epoch

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077676.jpg)

MOCO非常省内存只需要5个G，而且训练只需要53个小时（imagenet这种规模的数据集上，两天多已经算很快了）

端到端（InvaSpread、SimCLR）：SimCLR在小bs的时候效果远不如MOCOv2（因为字典不够大，负样本不够多，导致对比学习的对比不是很有效，而且不仅效果低内存占用也明显高，训练时长也长了十几个小时，端到端要性能差不多bs就要4096，这对硬件要求太高了）

36:20 SimCLRv2 Neural IPS

Big Self-Supervised Models are Strong Semi-Supervised Learners

非常大的自监督模型非常适合去做半监督学习

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077687.jpg)

1.  第一部分：SimCLRv2，怎样自监督或自监督的对比学习，去训练一个大的模型出来。
2.  第二部分：一旦有了一个很好的模型，只需要一小部分有标签的数据，去做一下有监督的微调。
3.  第三部分：微调结束了就相当于有了一个teacher模型，就可以用这个teacher模型去生成很多伪标签，这样就可以在更多的无标签数据上做自学习了。

我们要看的是第一部分，看作者怎么把SimCLR改进了。

1.  用更大的模型，无监督训练就会训练的更好。（152层的残差网络，selective kernels网络，骨干网络变得非常强）。
2.  SimCLR证明了projection head很有用（fc relu），那么变深可能会更有用，经过验证发现两层就够了（fc relu fc relu）。
3.  动量编码器在SimCLR就提升了一个点，可能是因为它本身已经有很大的bs了（4096或8192），负样本已经相当多了。

40:30SwAV

Unsupervised Learning of Visual Features by Contrasting Cluster Assignments

给定同一张图片，如果去生成不同视角的话，希望可以用一个视角得到的特征去预测另一个视角得到的特征，因为所有视角得到的特征应该都是非常接近的。

本文方法：对比学习+聚类。

聚类也是一种无监督表征方式，希望相似的物体都聚集在某一个聚类中心，不相似的物体尽量推开到别的聚类中心，所以和对比学习的方法也比较接近。

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077848.jpg)

1.  左边是过去的对比学习方法：图片经过不同的数据增强，通过编码器得到特征，然后对特征做一个对比学习的loss。
2.  右边是SwAV：认为特征和特征作对比，有点费资源，因为每个图片都是自己的类，那么剩下的都是负样本，负样本太大只能取近似，能不能不做近似？能不能借助一些先验信息，不去和大量的负样本比，而去跟一些更简洁的东西比呢？

去跟聚类中心比，就是右图中的 prototypes C（矩阵，维度是D×K，D是特征的维度，K聚类中心的个数）

前向过程：一个minibatch的图片，做两次数据增强，分别通过编码器得到两个特征，让特征和prototype C去生成一个目标，也就是Q1和Q2（相当于ground truth）。

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077773.jpg)

Swapped prediction：按道理x1和x2是同一张图片，他们是一对正样本，那么生成的特征应该很相似，Z1·C可以预测Q2，点乘的结果就是预测；ground truth就是聚类分类得到的Q1和Q2。通过换位预测的方法，SwAV可以对模型进行训练。

用聚类的好处是什么？

1.  如果要和很多的负样本作类比，可能就需要成千上万的负样本，即使如此，也只是个近似。而现在如果只是和聚类中心对比，用几百甚至3000个聚类中心就足以表示了，因为其实也并没有那么多类，imagenet也就1000类，COCO才80类，所以3000个聚类中心就足够用了，这相当于几万个负样本来说，是小了很多的。
2.  这些聚类中心是有明确的语义含义的，之前只是随机抽样负样本做对比的话，可能类别不均衡甚至可能是个正样本的，所以不如使用聚类中心有效。

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077916.jpg)

不仅比之前讲过的方法效果好，还要比接下来要讲的BYOL和SimSiam效果都要好。算是卷积神经网络里，用res50分刷的最高的一篇工作了。

对比做的是imagenet上的Linear classification，就是提前预训练好的模型，冻住主干网络，只训练最后的分类头（MLP层）。前六个不是对比学习的方法，结果都比较低。有了对比学习开始，MOCO就开始上60了。

SwAV是把主干网络冻住情况下做的，都已经非常逼近从头到尾都在imagenet上训练的有监督基线模型。

右图是只把res50变宽，SwAV的结果还能不停的涨。当用5倍的模型的时候，SwAV的结果已经和有监督的模型差距非常小。

SwAV的性能这么好的原因：

1.  和聚类的方法融合。
2.  multi-crop：一个trick。之前的对比学习方法用的是两个crop，就是一个正样本对就是两个图片x1和x2，如图所示，图片先resize到256*256，然后随机crop两个224*224的图片当成x1和x2，因为两张图片都非常大，所以重叠的区域也非常多，他们代表一个正样本，总之就是两个crop。这么大的crop抓住的是整个场景的特征，如果想学习局部物体的特征该怎么办？

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077678.jpg)

所以最好能多个crop，就能关注到局部的物体了。但是增加crop（文中的view）模型的计算复杂度一下就提高了，相当于使用了更多的正样本，如何能使用更多的正样本，而又不增加更多的计算成本？

方法：把crop变小，取两个较大的crop争取学到全局特征，然后为了增加正样本的数量，为了学习局部特征，再去随机选4个小一点的crop。正样本数量增多了，但是通过取舍，整体的计算代价是差不多的。

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077683.jpg)

这个multi-crop技术很有用，而且不仅对SwAV有效。

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077839.jpg)

如图，基线模型就是2*224，用了multi-crop技术的效果。

*   SimCLR涨了2.4个点。
*   聚类的方法用了这个技术，提点效果更显著。

如果不用multi-crop这个技术，SwAV的效果和mocov2是差不多的，也就是说一个纯聚类的方法，或者聚类和对比学习结合的方法，其实并没有什么优势，真正提点的是multi-crop这个技术，而且这个技术非常普适，思想也很简单，就是全局和局部的特征都要关注。所以接下来的很多工作借鉴的都是multi-crop这个技术，而不是SwAV这篇工作本身。

50:03

*   CPCv2：用了更大的模型、用了更大的图像块、做了更多方向上的预测任务、把BN换成了LN、使用了更多的数据增强
*   infoMin：What Makes for Good Views for Contrastive Learning，到底选择什么样的视角才能对对比学习更好。infomin原则：最小化互信息，以前都是最大化互信息，都是想要两个视角之间的互信息达到最大，本文是想要恰好合适的互信息，如果最后互信息比你所需要的互信息要多，那也是一种浪费，而且有可能泛化做的不好。如果互信息比需要的少，可能就达不到最优的性能。所以作者的意思就是，不能一味的最大化互信息，而是要不多不少刚刚好，按照infomin的原则，去选择合适的数据增强，然后拿到合适的对比学习视角，作者发现对于很多的方法都有提升。

第二阶段，很多细节已经趋于统一了

*   目标函数都是infoNCE或者其变体
*   模型都是用一个编码器后面加一个projection head
*   更强的数据增强
*   动量编码器
*   训练的更久
*   准确度逐渐逼近有监督的基线模型

52:07第三阶段：transformer，不用负样本的对比学习。

第二阶段讲的SwAV已经有这个趋势了，算是承上启下的工作，因为他也没有用负样本，用的是聚类中心，但还是有一个明确的对比的对象的。但这个阶段的工作，已经没有负样本或者聚类中心了，就是正样本自己在玩。

52:34BYOL：自己跟自己学

Bootstrap Your Own Latent

A New Approach to Self-Supervised Learning

*   Bootstrap：在一定基础上改造
*   Latent/hidden/feature/embedding：特征

为什么不用负样本这么新奇？

因为在对比学习中负样本是一个约束，在算目标函数的时候，只有正样本，那么目标只有一个：让所有相似的物体的特征也尽可能的相似，这时候就有一个很明显的捷径解，就是说如果一个模型，不论什么样的输入，都会返回相同的输出，那么出来的所有特征都是一模一样的，那这个去算对比学习的loss就都是0，意思就是模型直接躺平了根本不用学，直接用这个捷径解trivial solution，就能完美解决你的问题。只有加上负样本这个约束，不光相似的物体要有相似的特征，不相似的物体要有不相似的特征，这样模型才有动力继续学，因为如果输出的所有特征都一样，那么负样本这边loss就无穷大，所以模型必须想办法让正样本和负样本的loss都往下降，达到一个最优解。所以负样本在对比学习里是个必须的东西，能防止模型学到这个捷径解（model collapse或learning collapse）。

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077774.jpg)

前向过程：

*   一个minibatch的输入x
*   经过两次的数据增强后得到了v和v'
*   通过编码器得到特征，两个编码器使用同样的架构，但是参数不同，上侧的参数随梯度更新而更新，下侧编码器用动量更新的方式更新参数
*   projection head，通过g函数来降维，网络结构相同参数不同，下侧动量更新。得到两个特征。
*   BYOL在上侧加了一个层（MLP）得到一个新的特征。让这个新的特征和下侧特征尽可能一致，把一个匹配问题变成了一个预测问题。（和SwAV有点像，就是换成预测问题，但是借助了聚类中心去帮助做预测任务）相当于用一个视角的特征去预测另一个视角的特征。通过这种预测性的任务去完成模型的训练。

代理任务是对比学习为了预训练模型所定义的任务，为了训练模型。

*   个体判别任务：自成一类，让两个特征尽可能接近。
*   预测型的任务：预测对比

和其他工作同样的，训练完成之后只留下编码器，用上侧一次编码的特征yθ去做下游任务

目标函数：MSE loss，让预测的特征和下侧的特征尽可能的接近。

58:23博客复现BYOL，因为BN的一个细节复现不出来

SimCLR的projection head的细节：fc bn relu fc bn第一个全连接层2048*2048，第二个全连接层2048*128就把维度降下去了，这个128的特征就是最后用来做对比学习的那个特征。

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077680.jpg)

MOCOv2的projection head细节：fc relu fc 2048*2048→2048*128

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077942.jpg)

BYOL的projection head细节：fc BN relu fc 2048*2048→2048*128

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077847.jpg)

这篇博客复现借鉴了写的非常漂亮的MOCOv2代码，MLP中少了一个BN，模型的学习就坍塌了

projection MLP norm：projector中是否用BN，还是其他归一化方式

prediction MLP norm：训练中是否用BN，还是其他归一化方式

Loss function：目标函数

contrastive：对比

*   random：随机初始化的残差网络，用这个残差网络抽特征，在特征上训练一个全连接层，最后的结果是28.8。随机结果（没有学习）
*   BYOL：如果做的是正确的BYOL，就是在projection head和prediction head中都用了BN，结果是57.7最高。
*   no normalization：这是他们做错了的，在projection head和prediction head中都没有用BN，结果只有28.3，就是模型坍塌了什么都没学到
*   仅在projection head中用BN，和仅在prediction head中用BN，效果都还可以，说明模型没有坍塌，有在学习。
*   如果把projection head和prediction head中的BN换成LN，效果又不行了

作者就知道BYOL模型没有坍塌肯定和BN有关系：

注：BN是什么，就是把一个batch里所有样本的特征拿来算均值和方差（running mean和running variance）。然后用这个整个batch算来的均值和方差去做归一化，在算某个样本的loss的时候，其实也看到了其他样本的特征，这里面是有泄露的。MOCO中就做了一个shuffle BN，为了防止信息泄露。

作者说因为BN中有信息泄露，所以你可以把batch里的其他样本想成一种隐式的负样本。换句话说，当你有了bn的时候，BYOL其实并不是只是正样本在自己和自己学，其实也在做对比，BYOL做的对比任务就是：当前正样本的图片，和BN产生的平均图片的差别（这和SwAV的不用负样本而是聚类中心去对比是相似的，就是一种聚类中心的意思）

BYOL的作者就急了，因为如果这么解释的话，BYOL就是还是没有逃脱对比学习的范畴，于是BYOL就做了其他实验证明为什么模型没有坍塌。

01:05:32BYOL works even without batch statistics：BYOL即使没有BN也可以工作，想说明BYOL的成功不是因为BN

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077981.jpg)

BYOL是基于SimCLR上做的，这个详细的消融实验，

1.  BN确实比较关键，没有BN的BYOL确实就不学了，但SimCLR还在学
2.  有一组实验：Projector有BN，BYOL还是训练失败了，这就不能说明BN关键了。因为如果BN能提供隐式负样本的话，这里训练就不应该失败。
3.  当编码器和Projector都没有BN的时候，SimCLR也失败了，因为SimCLR没有predictor所以无所谓的。说明没有归一化的时候，不光BYOL，SimCLR也不行了，即使用了负样本也训练不出来。这就说明BN不是提供了一个隐式负样本，因为SimCLR提供了显式的负样本也训练不出来。

于是BYOL作者和博客作者达成一致：BN跟它原来的设计初衷一样，主要的作用是帮助模型稳定训练，提高模型训练的稳健性，让模型不会坍塌。

BYOL在本文的3.3中给出解释：如果一开始就能让模型初始化的比较好，那么后面的训练即使离开了BN也没问题。

weight standardization ：一种模型初始化的方式

group norm：一种归一化的方式

这两个方法是VIT的原班作者在它们之前的论文BEIT中提出来的，Resnet v2就是用这种方式做训练。

BYOL(+GN+wS) achieves 73.9%：BYOL换上GN和WS之后也能达到用BN的结果，而这两个方法都没有计算batch统计量。所以这个BYOL是没有跟minibatch中的其他样本做对比的，也就是没有隐式对比。说明BYOL还是一个全新的方式，能自己跟自己学就能学的很好。

01:09:35Exploring Simple Siamese Representation Learning，SimSiam对对比学习进行分析，化繁为简

SimSiam

1.  不用负样本，基本上和BYOL很相似
2.  不需要大的bs
3.  不需要动量编码器

不仅不会模型坍塌，而且效果还很好。

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257078047.jpg)

为什么叫孪生网络：两个编码器网络结构相同且共享参数，整体架构和BYOL非常像，不同只在于SimSiam没有用动量编码器。

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257078103.jpg)

前向过程：得到两个视角之后先过编码器得到特征，然后通过predictor去得到预测，两边都预测，所以是一个对称性的loss：既可以做p1预测z2，也可以做p2预测z1的任务，但因为加了两次，所以要除以2

然后就是梯度回传更新网络

D函数表示loss的计算：MSEloss

结论：之所以SimSiam可以训练，不会模型坍塌，主要就是因为有stop gradient这个操作

做了一个假设：因为有了stop gradient这个操作，所以可以把SimSiam想象成一个expectation-maximization算法。因为有了stop gradient这个操作，这一个训练过程或者说这一套模型参数其实就被人为的劈成了两份，相当于在解决两个子问题，模型的更新也是在交替进行的。

可以理解成一个k-means的聚类问题，Kmeans就是分两步走的，每次先要把所有的点分配给一些聚类中心，分配好了以后再去更新聚类中心，再重复做这个操作。从这个角度说，SimSiam和SwAV就有关系了。如图归纳总结了所有孪生网络的做法

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257078091.jpg)

1.  SimCLR两边都有梯度回传，做的是一个对比任务
2.  SwAV也是做对比任务，但并没有跟负样本比，而是和聚类中心比，聚类中心是通过SK算法得到的
3.  BYOL不是对比任务，加了predictor变成一个预测任务，用左边去预测右边，同时他们还使用了动量编码器
4.  SimSiam的整个左边和BYOL一模一样，不同的是右边没有用动量编码器而是共享参数了

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257078112.jpg)

在imagenet上的Linear classification

*   只用MOCOv2和SimSiam能用256的bs，其他的工作都要用更大的bs
*   SimCLR和MOCOv2都要用负样本，BYOL完全没用，SwAV用的聚类中心
*   100epoch的时候SimSiam学的最好，说明学的很快，但是随着训练推进涨幅就小了，动量编码器很好用，很好提点。但是本文主要是把这些trick都拿掉，证明没有这些trick也能训练。

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257078025.jpg)

下游任务上，前三个是物体检测，最后一个实例分割（CV人必做的两个下游任务）

*   针对下游任务的迁移来说，MOCOv2和SimSiam是表现最好的。如果想尝试一些对比学习的工作，会用MOCOv2作为基线模型，因为训练快训练稳而且下游任务迁移的好。

01:17:14第四阶段：transformer怎么和对比学习有机结合起来

01:17:23 mocov3An Empirical Study of Training Self-Supervised Vision Transformers 自监督的vision transformer，mocov3只是一种架构，卷积神经网络也可以用vision transformer也可以用。

贡献：做了一个很直接、很小的改动，让自监督的vit训练变得更稳定了

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077932.jpg)

mocov3其实相当于mocov2和SimSiam的合体

整体框架：query编码器（backbone+projection head+prediction head，就是BYOL/SimSiam）和key编码器（动量编码器）

目标函数：对称的，既算q2到k1，也算q1到k2，对比学习的loss

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077980.jpg)

因为vision transformer的出现，作者就很想把卷积神经网络换成vit，看看自监督学习+vit就能取得NLP那边的成功呢？

实验：把骨干网络从Resnet换成vit，如图是vit自监督训练的训练曲线，作者发现：

*   当bs比较小的时候，曲线比较平滑，效果也还行
*   当bs变大之后，准确度会突然掉下来又恢复，但是就不如原来的高了，最后的准确度也会差很多。按道理说大bs应该会有更好的结果，但是这里大bs的结果却只有69.7

如果能解决这个问题，有可能就能使用更大的bs去训练一个更大的vit，从而得到更好的结果。

4.2 针对这个普遍出现的问题，提出一个小trick，非常普适

如何想到的这个解决方式：观察训练的时候每一层的回传梯度的情况（一般网络训练的不好，不知道为什么的时候，一般首先就是要去查梯度）。作者发现每次loss有大幅的震动，导致准确度大幅下降的时候，梯度也会有一个波峰，这个波峰发生在第一层，就是做patch Projection的时候（patch Projection就是vit论文中的第一步，属于tokenization的阶段，就是如何把一个图片打成patch，然后给它一个特征，做法就是：一个可以训练的全连接层）这个可以训练的全连接层，每次梯度都不正常，那还不如不训练。所以作者就尝试把这个MLP冻住，看结果。（就是随机初始化了patch Projection层，然后就冻住，整个训练过程都不变）问题就解决了。

这个trick不仅对mocov3有用，对BYOL也有用（用BYOL的框架，把残差网络换成vit），把patch Projection层冻住也能获得更平滑的训练曲线，获得更好的训练结果。

因为transformer简单扩展性又好，不改它就只能改开始的tokenization阶段，和结尾的目标函数

01:23:12 dinoEmerging Properties in Self-Supervised Vision Transformers

也是自监督训练vit的方式，但是主要卖点在于vit在自监督训练的情况下出现的有趣特性

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077993.jpg)

*   一个完全不用标签信息训练出来的vit，如果把自注意力图拿出来进行可视化的话，会发现它能非常准确的抓住每个物体的轮廓，效果甚至能媲美直接对物体做分割

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077983.jpg)

dino具体做法：延续了BYOL

self-distillation：和BYOL一样自己和自己学

用student去预测teacher

centering：减掉batch样本的均值。为了避免模型坍塌，把整个batch里的样本算一个均值，减掉这个均值。

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257078085.jpg)

和MOCOv3非常像，前向过程一模一样，目标函数多了一个centering操作防止模型坍塌。

第四阶段，01:25:42c从方法和模型角度上讲，和第三阶段一模一样，主要是融合了transformer

01:26:01 整体过一遍

![](https://raw.githubusercontent.com/wang-akang/study/refs/heads/main/1750257077915.jpg)

第一阶段：

*   InstDisc提出了个体判别任务，提出用一个memory bank的外部数据结构去存储负样本，从而得到一个又大又一致的字典去做对比学习
*   InvaSpread不用外部结构的另一条路，端到端学习。只用一个编码器，从而可以端到端学习，但因为受限于bs太小，所以性能不够好。
*   CPCv1提出了infoNCEloss，CPCv1是一个预测型的代理任务，不仅可以做图像，还可以做音频、视频、文字和强化学习，是一个非常全能的结构。
*   CMC把两个视角的任务扩展到了多个视角，给接下来的多视角或者多模态的对比学习打下了铺垫。
*   deep cluster是基于聚类学习的，当时还没有对比学习

第二阶段：

*   MOCOv1是InstDisc的延伸工作，把memory bank变成一个队列，把动量更新特征变成了动量更新编码器，从而能预训练一个很好的模型。moco也是第一个在很多视觉的下游任务上，让一个无监督预训练的模型比有监督预训练模型表现好的方法。属于使用外部数据结构的。
*   SimCLRv1是端到端的延伸性工作，和InvaSpread很像，但是用了很多的技术：加大bs、用了更多数据增强、加了一个Projection head、训练更长时间。所有的技术堆起来，让SimCLR在imagenet上取得了非常好的结果。
*   CPCv2，也把这些技术用了一遍，直接比CPCv1在imagenet上的结果高了三十几个点。
*   CMC把这些都分析了一下，提出了一个infomin的原则：两个视角之间的互信息要不多不少才是最好的

*   MOCOv2发现这些即插即用的技术效果很好，就拿来用。
*   SimCLRv2主要做半监督学习。
*   SwAV把聚类学习和对比学习结合起来的一个工作，取得了不错的效果，这个不错的结果主要来自于它提出的multicrop的技术，如果没有这个技术就和mocov2或者SimCLR结果差不多。

第三阶段：

*   BYOL提出处理负样本太麻烦，不要负样本了，不用负样本做对比了。把对比任务变成预测任务，自己跟自己学就行了。目标函数也很简单就是MSEloss就训练出来了
*   BN BLOG：说BYOL能工作是因为用BN，BN提供了隐式负样本，所以BYOL才能正常训练而不会模型坍塌。
*   BYOLv2：说BN只是帮助了模型训练，如果用另一种方式能提供更好的模型初始化，BYOL不需要BN提供的batch的统计量也可以工作。
*   SimSiam总结了之前的工作，因为之前的工作一直在堆技术，堆多了不好分析，领域就不好推进了。所以SimSiam化繁为简提出来一个很简单的孪生网络的学习方法，既不需要大的bs，也不需要动量编码器，也不需要负样本，照样能取得不错的结果。SImSiam提出的假设就是，stop gradient这个操作至关重要，因为有这个操作的存在，SImSiam可以看作是一种EM算法，通过逐步更新的方式避免模型坍塌。
*   barlos twins更换了一个目标函数，把之前大家做的对比和预测变成了两个矩阵之间比相似性。但因为是21年3月提出的，很快就淹没在vit的洪流之中。

第四阶段：这两个工作都是把骨干网络换成了vit。但是换成vit之后训练不稳定或者不好训练。他们提出了各自的解决方法。两种方法都能有效的提高模型训练的稳健性，防止模型坍塌，让vit用自监督的方式也能训练的很好。

*   MOCOv3：把patch Projection layer冻住
*   DINO：把teacher网络的输出先做一下归一化（centering）
